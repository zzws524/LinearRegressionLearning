{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(16, 1, 28, 28)\n",
      "0.0842\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#matplotlib.use('Agg')\n",
    "import d2l\n",
    "from mxnet import autograd,nd,gluon\n",
    "from IPython import display\n",
    "\n",
    "batch_size=256\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "num_inputs=784\n",
    "num_outputs=10\n",
    "\n",
    "W=nd.random.normal(scale=0.01,shape=(num_inputs,num_outputs))\n",
    "b=nd.zeros(num_outputs)\n",
    "\n",
    "W.attach_grad()\n",
    "b.attach_grad()\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    X_exp=X.exp()\n",
    "    partition=X_exp.sum(axis=1,keepdims=True)\n",
    "    return X_exp/partition\n",
    "\n",
    "def net(X):\n",
    "    return softmax(nd.dot(X.reshape((-1,num_inputs)),W)+b) #-1 means unspecified num. +b:propagation\n",
    "\n",
    "def cross_entropy(y_hat,y):\n",
    "    return -nd.pick(y_hat,y).log()\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(axis=1) == y.astype('float32')).sum().asscalar()\n",
    "\n",
    "def evaluate_accuracy(net,data_iter):\n",
    "    metric=d2l.Accumulator(2) # num_corrected_examples, num_examples\n",
    "    for X,y in data_iter:\n",
    "        #print(X.shape)  shape is (256, 1, 28, 28)\n",
    "        #print(y)  e.g. [1,0,6,9....7]\n",
    "        y=y.astype('float32')\n",
    "        metric.add(accuracy(net(X),y),y.size) #y.size = size of this batch, e.g.256\n",
    "    return metric[0]/metric[1]\n",
    "\n",
    "class Accumulator(object):\n",
    "    \"\"\"Sum a list of numbers over time\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "    def add(self, *args):\n",
    "        self.data = [a+b for a, b in zip(self.data, args)] #args is array, so it's array+array\n",
    "    def reset(self):\n",
    "        self.data = [0] * len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "\n",
    "print(evaluate_accuracy(net, test_iter)) \n",
    "\n",
    "# Save to the d2l package.\n",
    "def train_epoch_ch3(net, train_iter, loss, updater):\n",
    "    metric = Accumulator(3) # train_loss_sum, train_acc_sum, num_examples\n",
    "    if isinstance(updater, gluon.Trainer):\n",
    "        updater = updater.step\n",
    "    for X, y in train_iter:\n",
    "        # compute gradients and update parameters\n",
    "        with autograd.record():\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "        l.backward()\n",
    "        updater(X.shape[0])\n",
    "        metric.add(l.sum().asscalar(), accuracy(y_hat, y), y.size)\n",
    "    # Return training loss and training accuracy\n",
    "    return metric[0]/metric[2], metric[1]/metric[2]\n",
    "\n",
    "\n",
    "# Save to the d2l package.\n",
    "class Animator(object):\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=[], xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear', fmts=None,\n",
    "                 nrows=1, ncols=1, figsize=(3.5, 2.5)):\n",
    "        \"\"\"Incrementally plot multiple lines.\"\"\"\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1: self.axes = [self.axes,]\n",
    "        # use a lambda to capture arguments\n",
    "        self.config_axes = lambda : d2l.set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        \"\"\"Add multiple data points into the figure.\"\"\"\n",
    "        if not hasattr(y, \"__len__\"): y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"): x = [x] * n\n",
    "        if not self.X: self.X = [[] for _ in range(n)]\n",
    "        if not self.Y: self.Y = [[] for _ in range(n)]\n",
    "        if not self.fmts: self.fmts = ['-'] * n\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "# Save to the d2l package.\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):\n",
    "    trains, test_accs = [], []\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                        ylim=[0.3, 0.9],\n",
    "                        legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch+1, train_metrics+(test_acc,))\n",
    "       \n",
    "        \n",
    "num_epochs, lr = 10, 0.1\n",
    "updater = lambda batch_size: d2l.sgd([W, b], lr, batch_size)\n",
    "#train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)\n",
    "\n",
    "\n",
    "# Save to the d2l package.\n",
    "def predict_ch3(net, test_iter, n=6):\n",
    "    for X, y in test_iter:\n",
    "        break\n",
    "    trues = d2l.get_fashion_mnist_labels(y.asnumpy())\n",
    "    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1).asnumpy())\n",
    "    titles = [true+'\\n'+ pred for true, pred in zip(trues, preds)]\n",
    "    d2l.show_images(X[0:n].reshape((n,28,28)), 1, n, titles=titles[0:n])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#predict_ch3(net, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
